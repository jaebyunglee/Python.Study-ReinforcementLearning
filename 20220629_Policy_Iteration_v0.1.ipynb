{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "\n",
    "from tkinter import Button\n",
    "from PIL import ImageTk, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <Enviroment Class>\n",
    "PhotoImage = ImageTk.PhotoImage\n",
    "UNIT = 100  # 픽셀 수\n",
    "HEIGHT = 5  # 그리드월드 세로\n",
    "WIDTH = 5  # 그리드월드 가로\n",
    "TRANSITION_PROB = 1\n",
    "POSSIBLE_ACTIONS = [0, 1, 2, 3]  # 좌, 우, 상, 하\n",
    "ACTIONS = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # 좌표로 나타낸 행동\n",
    "REWARDS = []\n",
    "\n",
    "\n",
    "class Env:\n",
    "    def __init__(self):\n",
    "        self.transition_probability = TRANSITION_PROB\n",
    "        self.width = WIDTH\n",
    "        self.height = HEIGHT\n",
    "        self.reward = [[0] * WIDTH for _ in range(HEIGHT)]\n",
    "        self.possible_actions = POSSIBLE_ACTIONS\n",
    "        self.reward[2][2] = 1  # (2,2) 좌표 동그라미 위치에 보상 1\n",
    "        self.reward[1][2] = -1  # (1,2) 좌표 세모 위치에 보상 -1\n",
    "        self.reward[2][1] = -1  # (2,1) 좌표 세모 위치에 보상 -1\n",
    "        self.all_state = []\n",
    "\n",
    "        for x in range(WIDTH):\n",
    "            for y in range(HEIGHT):\n",
    "                state = [x, y]\n",
    "                self.all_state.append(state)\n",
    "\n",
    "    def get_reward(self, state, action):\n",
    "        next_state = self.state_after_action(state, action)\n",
    "        return self.reward[next_state[0]][next_state[1]]\n",
    "\n",
    "    def state_after_action(self, state, action_index):\n",
    "        action = ACTIONS[action_index]\n",
    "        return self.check_boundary([state[0] + action[0], state[1] + action[1]])\n",
    "\n",
    "    @staticmethod\n",
    "    def check_boundary(state):\n",
    "        state[0] = (0 if state[0] < 0 else WIDTH - 1\n",
    "                    if state[0] > WIDTH - 1 else state[0])\n",
    "        state[1] = (0 if state[1] < 0 else HEIGHT - 1\n",
    "                    if state[1] > HEIGHT - 1 else state[1])\n",
    "        return state\n",
    "\n",
    "    def get_transition_prob(self, state, action):\n",
    "        return self.transition_probability\n",
    "\n",
    "    def get_all_states(self):\n",
    "        return self.all_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <Environment Setting>\n",
    "env = Env()\n",
    "\n",
    "# <가치함수를 2차원 리스트로 초기화>\n",
    "# value table 생성\n",
    "value_table = np.zeros(env.width * env.height).reshape(env.width,env.height).tolist()\n",
    "\n",
    "# next value table 생성 (정책 평가 후 value table 을 next value table로 업데이트)\n",
    "# next_value_table = np.zeros(env.width * env.height).reshape(env.width,env.height).tolist()\n",
    "next_value_table = copy.deepcopy(value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <상하좌우 동일한 확률로 정책 초기화>\n",
    "policy_table = \\\n",
    "np.repeat(np.repeat(0.25,4), env.width * env.height).reshape(env.height, env.width,-1).tolist()\n",
    "\n",
    "# next_policy_table = \\\n",
    "# np.repeat(np.repeat(0.25,4), env.width * env.height).reshape(env.height, env.width,-1).tolist()\n",
    "next_policy_table = copy.deepcopy(policy_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <End 상태의 설정>\n",
    "policy_table[2][2] = []\n",
    "\n",
    "# <할인율 설정>\n",
    "discount_factor = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정책 평가를 여러번 한 후, 정책 발전 -> 이 과정을 반복해서 진행하면 최적 정책이 찾아짐\n",
    "```\n",
    "- state  : [x,y]\n",
    "- action : [0,1,2,3] - > [상,하,좌,우]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ state : [0, 0] ]\n",
      "action : 0 / policy : 0.25 / reward : 0 / next_state : [0, 0] \n",
      "action : 1 / policy : 0.25 / reward : 0 / next_state : [1, 0] \n",
      "action : 2 / policy : 0.25 / reward : 0 / next_state : [0, 0] \n",
      "action : 3 / policy : 0.25 / reward : 0 / next_state : [0, 1] \n",
      ">> value : 0.54 / next value : 0.54\n",
      "[ state : [0, 1] ]\n",
      "action : 0 / policy : 0.00 / reward : 0 / next_state : [0, 1] \n",
      "action : 1 / policy : 0.00 / reward : 0 / next_state : [1, 1] \n",
      "action : 2 / policy : 0.00 / reward : 0 / next_state : [0, 0] \n",
      "action : 3 / policy : 1.00 / reward : 0 / next_state : [0, 2] \n",
      ">> value : 0.66 / next value : 0.66\n",
      "[ state : [0, 2] ]\n",
      "action : 0 / policy : 0.00 / reward : 0 / next_state : [0, 2] \n",
      "action : 1 / policy : 0.00 / reward : -1 / next_state : [1, 2] \n",
      "action : 2 / policy : 0.00 / reward : 0 / next_state : [0, 1] \n",
      "action : 3 / policy : 1.00 / reward : 0 / next_state : [0, 3] \n",
      ">> value : 0.73 / next value : 0.73\n",
      "[ state : [0, 3] ]\n",
      "action : 0 / policy : 0.00 / reward : 0 / next_state : [0, 3] \n",
      "action : 1 / policy : 1.00 / reward : 0 / next_state : [1, 3] \n",
      "action : 2 / policy : 0.00 / reward : 0 / next_state : [0, 2] \n",
      "action : 3 / policy : 0.00 / reward : 0 / next_state : [0, 4] \n",
      ">> value : 0.81 / next value : 0.81\n",
      "[ state : [0, 4] ]\n",
      "action : 0 / policy : 0.00 / reward : 0 / next_state : [0, 4] \n",
      "action : 1 / policy : 1.00 / reward : 0 / next_state : [1, 4] \n",
      "action : 2 / policy : 0.00 / reward : 0 / next_state : [0, 3] \n",
      "action : 3 / policy : 0.00 / reward : 0 / next_state : [0, 4] \n",
      ">> value : 0.73 / next value : 0.73\n",
      "[ state : [1, 0] ]\n",
      "action : 0 / policy : 0.00 / reward : 0 / next_state : [0, 0] \n",
      "action : 1 / policy : 1.00 / reward : 0 / next_state : [2, 0] \n",
      "action : 2 / policy : 0.00 / reward : 0 / next_state : [1, 0] \n",
      "action : 3 / policy : 0.00 / reward : 0 / next_state : [1, 1] \n",
      ">> value : 0.66 / next value : 0.66\n",
      "[ state : [1, 1] ]\n",
      "action : 0 / policy : 0.00 / reward : 0 / next_state : [0, 1] \n",
      "action : 1 / policy : 0.50 / reward : -1 / next_state : [2, 1] \n",
      "action : 2 / policy : 0.00 / reward : 0 / next_state : [1, 0] \n",
      "action : 3 / policy : 0.50 / reward : -1 / next_state : [1, 2] \n",
      ">> value : -0.10 / next value : -0.10\n",
      "[ state : [1, 2] ]\n",
      "action : 0 / policy : 0.00 / reward : 0 / next_state : [0, 2] \n",
      "action : 1 / policy : 1.00 / reward : 1 / next_state : [2, 2] \n",
      "action : 2 / policy : 0.00 / reward : 0 / next_state : [1, 1] \n",
      "action : 3 / policy : 0.00 / reward : 0 / next_state : [1, 3] \n",
      ">> value : 1.00 / next value : 1.00\n",
      "[ state : [1, 3] ]\n",
      "action : 0 / policy : 0.00 / reward : 0 / next_state : [0, 3] \n",
      "action : 1 / policy : 1.00 / reward : 0 / next_state : [2, 3] \n",
      "action : 2 / policy : 0.00 / reward : -1 / next_state : [1, 2] \n",
      "action : 3 / policy : 0.00 / reward : 0 / next_state : [1, 4] \n",
      ">> value : 0.90 / next value : 0.90\n",
      "[ state : [1, 4] ]\n",
      "action : 0 / policy : 0.00 / reward : 0 / next_state : [0, 4] \n",
      "action : 1 / policy : 0.50 / reward : 0 / next_state : [2, 4] \n",
      "action : 2 / policy : 0.50 / reward : 0 / next_state : [1, 3] \n",
      "action : 3 / policy : 0.00 / reward : 0 / next_state : [1, 4] \n",
      ">> value : 0.81 / next value : 0.81\n",
      "[ state : [2, 0] ]\n",
      "action : 0 / policy : 0.00 / reward : 0 / next_state : [1, 0] \n",
      "action : 1 / policy : 1.00 / reward : 0 / next_state : [3, 0] \n",
      "action : 2 / policy : 0.00 / reward : 0 / next_state : [2, 0] \n",
      "action : 3 / policy : 0.00 / reward : -1 / next_state : [2, 1] \n",
      ">> value : 0.73 / next value : 0.73\n",
      "[ state : [2, 1] ]\n",
      "action : 0 / policy : 0.00 / reward : 0 / next_state : [1, 1] \n",
      "action : 1 / policy : 0.00 / reward : 0 / next_state : [3, 1] \n",
      "action : 2 / policy : 0.00 / reward : 0 / next_state : [2, 0] \n",
      "action : 3 / policy : 1.00 / reward : 1 / next_state : [2, 2] \n",
      ">> value : 1.00 / next value : 1.00\n",
      "[ state : [2, 2] ]\n",
      ">> value : 0.00 / next value : 0.00\n",
      "[ state : [2, 3] ]\n",
      "action : 0 / policy : 0.00 / reward : 0 / next_state : [1, 3] \n",
      "action : 1 / policy : 0.00 / reward : 0 / next_state : [3, 3] \n",
      "action : 2 / policy : 1.00 / reward : 1 / next_state : [2, 2] \n",
      "action : 3 / policy : 0.00 / reward : 0 / next_state : [2, 4] \n",
      ">> value : 1.00 / next value : 1.00\n",
      "[ state : [2, 4] ]\n",
      "action : 0 / policy : 0.00 / reward : 0 / next_state : [1, 4] \n",
      "action : 1 / policy : 0.00 / reward : 0 / next_state : [3, 4] \n",
      "action : 2 / policy : 1.00 / reward : 0 / next_state : [2, 3] \n",
      "action : 3 / policy : 0.00 / reward : 0 / next_state : [2, 4] \n",
      ">> value : 0.90 / next value : 0.90\n",
      "[ state : [3, 0] ]\n",
      "action : 0 / policy : 0.00 / reward : 0 / next_state : [2, 0] \n",
      "action : 1 / policy : 0.00 / reward : 0 / next_state : [4, 0] \n",
      "action : 2 / policy : 0.00 / reward : 0 / next_state : [3, 0] \n",
      "action : 3 / policy : 1.00 / reward : 0 / next_state : [3, 1] \n",
      ">> value : 0.81 / next value : 0.81\n",
      "[ state : [3, 1] ]\n",
      "action : 0 / policy : 0.00 / reward : -1 / next_state : [2, 1] \n",
      "action : 1 / policy : 0.00 / reward : 0 / next_state : [4, 1] \n",
      "action : 2 / policy : 0.00 / reward : 0 / next_state : [3, 0] \n",
      "action : 3 / policy : 1.00 / reward : 0 / next_state : [3, 2] \n",
      ">> value : 0.90 / next value : 0.90\n",
      "[ state : [3, 2] ]\n",
      "action : 0 / policy : 1.00 / reward : 1 / next_state : [2, 2] \n",
      "action : 1 / policy : 0.00 / reward : 0 / next_state : [4, 2] \n",
      "action : 2 / policy : 0.00 / reward : 0 / next_state : [3, 1] \n",
      "action : 3 / policy : 0.00 / reward : 0 / next_state : [3, 3] \n",
      ">> value : 1.00 / next value : 1.00\n",
      "[ state : [3, 3] ]\n",
      "action : 0 / policy : 0.50 / reward : 0 / next_state : [2, 3] \n",
      "action : 1 / policy : 0.00 / reward : 0 / next_state : [4, 3] \n",
      "action : 2 / policy : 0.50 / reward : 0 / next_state : [3, 2] \n",
      "action : 3 / policy : 0.00 / reward : 0 / next_state : [3, 4] \n",
      ">> value : 0.90 / next value : 0.90\n",
      "[ state : [3, 4] ]\n",
      "action : 0 / policy : 0.50 / reward : 0 / next_state : [2, 4] \n",
      "action : 1 / policy : 0.00 / reward : 0 / next_state : [4, 4] \n",
      "action : 2 / policy : 0.50 / reward : 0 / next_state : [3, 3] \n",
      "action : 3 / policy : 0.00 / reward : 0 / next_state : [3, 4] \n",
      ">> value : 0.81 / next value : 0.81\n",
      "[ state : [4, 0] ]\n",
      "action : 0 / policy : 0.00 / reward : 0 / next_state : [3, 0] \n",
      "action : 1 / policy : 0.00 / reward : 0 / next_state : [4, 0] \n",
      "action : 2 / policy : 0.00 / reward : 0 / next_state : [4, 0] \n",
      "action : 3 / policy : 1.00 / reward : 0 / next_state : [4, 1] \n",
      ">> value : 0.73 / next value : 0.73\n",
      "[ state : [4, 1] ]\n",
      "action : 0 / policy : 0.50 / reward : 0 / next_state : [3, 1] \n",
      "action : 1 / policy : 0.00 / reward : 0 / next_state : [4, 1] \n",
      "action : 2 / policy : 0.00 / reward : 0 / next_state : [4, 0] \n",
      "action : 3 / policy : 0.50 / reward : 0 / next_state : [4, 2] \n",
      ">> value : 0.81 / next value : 0.81\n",
      "[ state : [4, 2] ]\n",
      "action : 0 / policy : 1.00 / reward : 0 / next_state : [3, 2] \n",
      "action : 1 / policy : 0.00 / reward : 0 / next_state : [4, 2] \n",
      "action : 2 / policy : 0.00 / reward : 0 / next_state : [4, 1] \n",
      "action : 3 / policy : 0.00 / reward : 0 / next_state : [4, 3] \n",
      ">> value : 0.90 / next value : 0.90\n",
      "[ state : [4, 3] ]\n",
      "action : 0 / policy : 0.50 / reward : 0 / next_state : [3, 3] \n",
      "action : 1 / policy : 0.00 / reward : 0 / next_state : [4, 3] \n",
      "action : 2 / policy : 0.50 / reward : 0 / next_state : [4, 2] \n",
      "action : 3 / policy : 0.00 / reward : 0 / next_state : [4, 4] \n",
      ">> value : 0.81 / next value : 0.81\n",
      "[ state : [4, 4] ]\n",
      "action : 0 / policy : 0.50 / reward : 0 / next_state : [3, 4] \n",
      "action : 1 / policy : 0.00 / reward : 0 / next_state : [4, 4] \n",
      "action : 2 / policy : 0.50 / reward : 0 / next_state : [4, 3] \n",
      "action : 3 / policy : 0.00 / reward : 0 / next_state : [4, 4] \n",
      ">> value : 0.73 / next value : 0.73\n"
     ]
    }
   ],
   "source": [
    "# <[정책평가] 벨만 기대 방정식을 통해 다음 가치함수를 계산>\n",
    "\n",
    "# state = env.get_all_states()[0]\n",
    "for state in env.get_all_states() :\n",
    "    print(f'[ state : {state} ]')\n",
    "    value = 0\n",
    "    # End 위치([2,2])이면 value = 0 아니면 value 계산\n",
    "    if state != [2,2] :\n",
    "        for action in env.possible_actions :\n",
    "            next_state = env.state_after_action(state,action)\n",
    "            reward = env.get_reward(state, action)\n",
    "            next_value = value_table[next_state[0]][next_state[1]]\n",
    "            value += policy_table[state[0]][state[1]][action] * (reward + discount_factor * next_value) # 기대방정식 가치함수(70 page)\n",
    "            print(f'action : {action} / policy : {policy_table[state[0]][state[1]][action]:.2f} / reward : {reward} / next_state : {next_state} ')\n",
    "\n",
    "    next_value_table[state[0]][state[1]] = value  \n",
    "    \n",
    "    tmp_value      = f'{value_table[state[0]][state[1]] :.2f}'\n",
    "    tmp_next_value = f'{next_value_table[state[0]][state[1]] :.2f}'\n",
    "    print(f'>> value : {tmp_value} / next value : {tmp_next_value}')\n",
    "          \n",
    "# value_table 을 next_value_table로 업데이트\n",
    "value_table = copy.deepcopy(next_value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state : [0, 0] / policy : ['0.00', '0.50', '0.00', '0.50'] / next policy : ['0.00', '0.50', '0.00', '0.50']\n",
      "state : [0, 1] / policy : ['0.00', '0.00', '0.00', '1.00'] / next policy : ['0.00', '0.00', '0.00', '1.00']\n",
      "state : [0, 2] / policy : ['0.00', '0.00', '0.00', '1.00'] / next policy : ['0.00', '0.00', '0.00', '1.00']\n",
      "state : [0, 3] / policy : ['0.00', '1.00', '0.00', '0.00'] / next policy : ['0.00', '1.00', '0.00', '0.00']\n",
      "state : [0, 4] / policy : ['0.00', '0.50', '0.50', '0.00'] / next policy : ['0.00', '0.50', '0.50', '0.00']\n",
      "state : [1, 0] / policy : ['0.00', '1.00', '0.00', '0.00'] / next policy : ['0.00', '1.00', '0.00', '0.00']\n",
      "state : [1, 1] / policy : ['0.50', '0.00', '0.50', '0.00'] / next policy : ['0.50', '0.00', '0.50', '0.00']\n",
      "state : [1, 2] / policy : ['0.00', '1.00', '0.00', '0.00'] / next policy : ['0.00', '1.00', '0.00', '0.00']\n",
      "state : [1, 3] / policy : ['0.00', '1.00', '0.00', '0.00'] / next policy : ['0.00', '1.00', '0.00', '0.00']\n",
      "state : [1, 4] / policy : ['0.00', '0.50', '0.50', '0.00'] / next policy : ['0.00', '0.50', '0.50', '0.00']\n",
      "state : [2, 0] / policy : ['0.00', '1.00', '0.00', '0.00'] / next policy : ['0.00', '1.00', '0.00', '0.00']\n",
      "state : [2, 1] / policy : ['0.00', '0.00', '0.00', '1.00'] / next policy : ['0.00', '0.00', '0.00', '1.00']\n",
      "state : [2, 2] / policy : ['0.00', '0.50', '0.00', '0.50'] / next policy : ['0.00', '0.50', '0.00', '0.50']\n",
      "state : [2, 3] / policy : ['0.00', '0.00', '1.00', '0.00'] / next policy : ['0.00', '0.00', '1.00', '0.00']\n",
      "state : [2, 4] / policy : ['0.00', '0.00', '1.00', '0.00'] / next policy : ['0.00', '0.00', '1.00', '0.00']\n",
      "state : [3, 0] / policy : ['0.00', '0.00', '0.00', '1.00'] / next policy : ['0.00', '0.00', '0.00', '1.00']\n",
      "state : [3, 1] / policy : ['0.00', '0.00', '0.00', '1.00'] / next policy : ['0.00', '0.00', '0.00', '1.00']\n",
      "state : [3, 2] / policy : ['1.00', '0.00', '0.00', '0.00'] / next policy : ['1.00', '0.00', '0.00', '0.00']\n",
      "state : [3, 3] / policy : ['0.50', '0.00', '0.50', '0.00'] / next policy : ['0.50', '0.00', '0.50', '0.00']\n",
      "state : [3, 4] / policy : ['0.50', '0.00', '0.50', '0.00'] / next policy : ['0.50', '0.00', '0.50', '0.00']\n",
      "state : [4, 0] / policy : ['0.50', '0.00', '0.00', '0.50'] / next policy : ['0.50', '0.00', '0.00', '0.50']\n",
      "state : [4, 1] / policy : ['0.50', '0.00', '0.00', '0.50'] / next policy : ['0.50', '0.00', '0.00', '0.50']\n",
      "state : [4, 2] / policy : ['1.00', '0.00', '0.00', '0.00'] / next policy : ['1.00', '0.00', '0.00', '0.00']\n",
      "state : [4, 3] / policy : ['0.50', '0.00', '0.50', '0.00'] / next policy : ['0.50', '0.00', '0.50', '0.00']\n",
      "state : [4, 4] / policy : ['0.50', '0.00', '0.50', '0.00'] / next policy : ['0.50', '0.00', '0.50', '0.00']\n"
     ]
    }
   ],
   "source": [
    "# <[정책발전] 현재 가치함수(value_table)에 대해서 탐욕 정책 발전>\n",
    "# state = env.get_all_states()[0]\n",
    "\n",
    "for state in env.get_all_states() :\n",
    "    \n",
    "    value_list = []\n",
    "    result = [0,0,0,0]\n",
    "\n",
    "    # 현재 state에서 각 action에 대한 Q - Value 계산\n",
    "    for action in env.possible_actions :\n",
    "        next_state = env.state_after_action(state,action)\n",
    "        reward = env.get_reward(state, action)\n",
    "        next_value = value_table[next_state[0]][next_state[1]]\n",
    "        value = reward + discount_factor * next_value # 큐함수 (74 page)\n",
    "        value_list.append(value)    \n",
    "        \n",
    "    # 가장 큰 Q - Value를 가진 행동의 개수 구하기\n",
    "    max_idx_list = np.argwhere(value_list == np.amax(value_list))\n",
    "    max_idx_list = max_idx_list.flatten().tolist()\n",
    "\n",
    "    # 행동 확률 계산 -> 가장 큰 Q - Value가 1개라면 Prob = 1\n",
    "    prob = 1/len(max_idx_list)\n",
    "\n",
    "    # 정책 발전 -> 가장 큰 Q - Value를 가진 행동에 새로 계산된 prob 대입, 나머지는 0\n",
    "    for idx in max_idx_list :\n",
    "        result[idx] = prob\n",
    "\n",
    "    # next policy table의 해당 state의 정책에 대입    \n",
    "    next_policy_table[state[0]][state[1]] = result    \n",
    "    \n",
    "    tmp_po_tab      = [f'{s :.2f}' for s in policy_table[state[0]][state[1]]]\n",
    "    tmp_next_po_tab = [f'{s :.2f}' for s in next_policy_table[state[0]][state[1]]]\n",
    "    print(f'state : {state} / policy : {tmp_po_tab} / next policy : {tmp_next_po_tab}')\n",
    "    \n",
    "# policy table을 next policy table로 지정    \n",
    "policy_table = copy.deepcopy(next_policy_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
